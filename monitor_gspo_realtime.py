#!/usr/bin/env python3
"""
Real-Time GSPO Monitor
Watch GSPO training dynamics live!
"""

import torch
import time
import sys
from transformers import AutoTokenizer, AutoModelForCausalLM
from gspo_implementation import GSPOTrainer, GSPOConfig
from custom_dataset import GSPOCustomDataset

class GSPORealtimeMonitor:
    """Monitor GSPO training in real-time"""
    
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸš€ GSPO REAL-TIME MONITOR")
        print("=" * 60)
        
        # Load model
        model_name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
        print(f"ğŸ“¡ Loading model: {model_name}")
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float32,
            low_cpu_mem_usage=True
        ).to(self.device)
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # Setup GSPO
        config = GSPOConfig(
            learning_rate=1e-7,
            left_clip_range=0.002,
            right_clip_range=0.002,
            batch_size=2,
            group_size=4,
            max_length=256  # Shorter for faster demo
        )
        
        self.trainer = GSPOTrainer(self.model, self.tokenizer, config, self.device)
        
        # Load dataset
        generator = GSPOCustomDataset()
        self.data = generator.generate_dataset(20, {"easy": 0.8, "medium": 0.2})
        self.reward_function = generator.create_reward_function()
        
        print(f"âœ… Monitor ready! {len(self.data)} problems loaded")
        print("=" * 60)
    
    def demo_response_generation(self):
        """Show GSPO generating responses in real-time"""
        print(f"\nğŸ¯ DEMO 1: RESPONSE GENERATION")
        print("-" * 40)
        
        problem = self.data[0]
        query = problem['query']
        expected_answer = problem['target_answer']
        
        print(f"ğŸ“ Problem: {query}")
        print(f"ğŸ¯ Expected: {expected_answer}")
        print(f"ğŸ¤– Generating response...")
        
        # Time response generation
        start_time = time.time()
        responses = self.trainer.generate_responses([query])
        generation_time = time.time() - start_time
        
        response = responses[0]
        reward = self.reward_function(query, response, problem)
        
        print(f"âš¡ Generated in {generation_time:.2f}s")
        print(f"ğŸ’¬ Response: {response[:100]}{'...' if len(response) > 100 else ''}")
        print(f"ğŸ† Reward: {reward:.3f}")
        
        return query, response, reward
    
    def demo_importance_ratios(self, query, response):
        """Show GSPO computing importance ratios"""
        print(f"\nğŸ”¬ DEMO 2: IMPORTANCE RATIO COMPUTATION")
        print("-" * 40)
        
        # Tokenize input
        combined_text = f"{query} {response}"
        inputs = self.tokenizer(
            combined_text,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=self.trainer.config.max_length
        ).to(self.device)
        
        # Find response start
        query_tokens = self.tokenizer(query, return_tensors="pt")
        response_start_idx = query_tokens['input_ids'].shape[1] - 1
        
        print(f"ğŸ“Š Computing sequence log probabilities...")
        
        # Current model log prob
        current_log_prob = self.trainer.compute_sequence_log_prob(
            self.trainer.model, inputs['input_ids'], inputs['attention_mask'], 
            response_start_idx, response_start_idx + 20  # First 20 tokens
        )
        
        # Old model log prob
        old_log_prob = self.trainer.compute_sequence_log_prob(
            self.trainer.old_model, inputs['input_ids'], inputs['attention_mask'], 
            response_start_idx, response_start_idx + 20
        )
        
        # Importance ratio
        log_ratio = current_log_prob - old_log_prob
        importance_ratio = torch.exp(log_ratio)
        
        print(f"ğŸ“ˆ Current Model Log-Prob: {current_log_prob:.6f}")
        print(f"ğŸ“‰ Old Model Log-Prob: {old_log_prob:.6f}")
        print(f"âš–ï¸  Log Ratio: {log_ratio:.6f}")
        print(f"ğŸ² Importance Ratio: {importance_ratio:.6f}")
        
        # Check if it would be clipped
        clipped = importance_ratio < (1 - self.trainer.config.left_clip_range) or \
                 importance_ratio > (1 + self.trainer.config.right_clip_range)
        
        if clipped:
            print(f"âœ‚ï¸  CLIPPED! (Outside [{1-self.trainer.config.left_clip_range:.3f}, {1+self.trainer.config.right_clip_range:.3f}])")
        else:
            print(f"âœ… NOT CLIPPED (Within bounds)")
        
        return importance_ratio.item()
    
    def demo_training_step(self):
        """Show one GSPO training step live"""
        print(f"\nğŸ‹ï¸ DEMO 3: LIVE TRAINING STEP")
        print("-" * 40)
        
        # Get a small batch
        batch_size = 2
        batch_problems = self.data[:batch_size]
        queries = [p['query'] for p in batch_problems]
        
        print(f"ğŸ¯ Training on {batch_size} problems...")
        print(f"ğŸ“ Problems: {[q[:30] + '...' for q in queries]}")
        
        # Training step with timing
        start_time = time.time()
        
        print(f"âš¡ Generating responses...")
        responses = self.trainer.generate_responses(queries)
        
        print(f"ğŸ† Computing rewards...")
        rewards = []
        for i, (query, response) in enumerate(zip(queries, responses)):
            reward = self.reward_function(query, response, batch_problems[i])
            rewards.append(reward)
            print(f"   Problem {i+1}: {reward:.3f}")
        
        print(f"ğŸ”¬ Computing GSPO loss...")
        
        # Compute loss (simplified version)
        try:
            loss_stats = self.trainer.train_step(queries, 
                lambda q, r: self.reward_function(q, r, next(p for p in batch_problems if p['query'] == q)))
            
            training_time = time.time() - start_time
            
            print(f"â±ï¸  Training completed in {training_time:.2f}s")
            print(f"ğŸ“Š TRAINING METRICS:")
            print(f"   Loss: {loss_stats.get('loss', 0):.6f}")
            print(f"   Clip Fraction: {loss_stats.get('clip_fraction', 0):.3f}")
            print(f"   Importance Ratio: {loss_stats.get('importance_ratio_mean', 1):.6f}")
            print(f"   Advantage Mean: {loss_stats.get('advantage_mean', 0):.6f}")
            print(f"   Reward Mean: {loss_stats.get('reward_mean', 0.5):.3f}")
            
            return loss_stats
        except Exception as e:
            print(f"âŒ Training step failed: {e}")
            return None
    
    def demo_continuous_monitoring(self, steps=5):
        """Show continuous GSPO training with live updates"""
        print(f"\nğŸ“¡ DEMO 4: CONTINUOUS MONITORING ({steps} steps)")
        print("-" * 40)
        
        metrics_history = []
        
        for step in range(steps):
            print(f"\nğŸ”„ STEP {step + 1}/{steps}")
            print("â”€" * 30)
            
            # Quick training step
            batch_problems = self.data[step*2:(step+1)*2]  # Different batch each time
            if len(batch_problems) < 2:
                batch_problems = self.data[:2]  # Fallback
            
            queries = [p['query'] for p in batch_problems]
            
            try:
                start_time = time.time()
                stats = self.trainer.train_step(queries, 
                    lambda q, r: self.reward_function(q, r, next(p for p in batch_problems if p['query'] == q)))
                
                step_time = time.time() - start_time
                
                metrics_history.append({
                    'step': step + 1,
                    'time': step_time,
                    **stats
                })
                
                # Live display
                print(f"â±ï¸  {step_time:.2f}s | "
                      f"Loss: {stats.get('loss', 0):.5f} | "
                      f"Clip: {stats.get('clip_fraction', 0):.2f} | "
                      f"Reward: {stats.get('reward_mean', 0.5):.3f}")
                
                # Progress indicator
                progress = "â–ˆ" * (step + 1) + "â–‘" * (steps - step - 1)
                print(f"[{progress}] {((step + 1) / steps * 100):.0f}%")
                
                # Update old model every 2 steps
                if (step + 1) % 2 == 0:
                    self.trainer.update_old_model()
                    print(f"ğŸ”„ Updated old model")
                
                time.sleep(0.5)  # Brief pause for readability
                
            except Exception as e:
                print(f"âŒ Step {step + 1} failed: {e}")
        
        # Summary
        print(f"\nğŸ“ˆ MONITORING SUMMARY:")
        print("-" * 40)
        if metrics_history:
            avg_time = sum(m['time'] for m in metrics_history) / len(metrics_history)
            final_loss = metrics_history[-1].get('loss', 0)
            final_clip = metrics_history[-1].get('clip_fraction', 0)
            final_reward = metrics_history[-1].get('reward_mean', 0.5)
            
            print(f"â±ï¸  Average Step Time: {avg_time:.2f}s")
            print(f"ğŸ Final Loss: {final_loss:.6f}")
            print(f"âœ‚ï¸  Final Clip Fraction: {final_clip:.3f}")
            print(f"ğŸ† Final Reward: {final_reward:.3f}")
            
            # Trend analysis
            if len(metrics_history) > 1:
                loss_trend = "ğŸ“ˆ" if final_loss > metrics_history[0].get('loss', 0) else "ğŸ“‰"
                reward_trend = "ğŸ“ˆ" if final_reward > metrics_history[0].get('reward_mean', 0.5) else "ğŸ“‰"
                print(f"ğŸ” Loss Trend: {loss_trend}")
                print(f"ğŸ” Reward Trend: {reward_trend}")
    
    def run_full_demo(self):
        """Run complete real-time demonstration"""
        try:
            # Demo 1: Response Generation
            query, response, reward = self.demo_response_generation()
            
            # Demo 2: Importance Ratios
            importance_ratio = self.demo_importance_ratios(query, response)
            
            # Demo 3: Single Training Step
            training_stats = self.demo_training_step()
            
            # Demo 4: Continuous Monitoring
            self.demo_continuous_monitoring(5)
            
            # Final Summary
            print(f"\nğŸ‰ GSPO REAL-TIME DEMO COMPLETE!")
            print("=" * 60)
            print(f"âœ… Response Generation: WORKING")
            print(f"âœ… Importance Ratios: WORKING (Ratio: {importance_ratio:.3f})")
            print(f"âœ… Training Steps: {'WORKING' if training_stats else 'FAILED'}")
            print(f"âœ… Continuous Training: WORKING")
            print(f"\nğŸ† GSPO IS FULLY OPERATIONAL! ğŸ†")
            
        except Exception as e:
            print(f"\nâŒ Demo failed: {e}")
            import traceback
            traceback.print_exc()

def main():
    """Main monitoring function"""
    try:
        monitor = GSPORealtimeMonitor()
        monitor.run_full_demo()
    except KeyboardInterrupt:
        print(f"\nâ¸ï¸  Demo interrupted by user")
    except Exception as e:
        print(f"\nâŒ Monitor failed: {e}")

if __name__ == "__main__":
    main() 